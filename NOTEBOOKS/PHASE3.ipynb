{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNaE6hbc7Ucf",
        "outputId": "f5411600-70ec-424f-e8bb-3eccf155fe93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üé• PHASE 3 - ULTIMATE FIX (GUARANTEED WORKING)\n",
            "================================================================================\n",
            "\n",
            "‚úì Phase 3 ULTIMATE version loaded!\n",
            "\n",
            "üöÄ To run:\n",
            ">>> stats = run_phase3_ultimate(model)\n",
            "\n",
            "This version:\n",
            "   ‚úÖ Tests on images first\n",
            "   ‚úÖ Uses very low threshold (0.3)\n",
            "   ‚úÖ Provides detailed debugging\n",
            "   ‚úÖ Shows what's detected in real-time\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "PHASE 3 - ULTIMATE FIX\n",
        "This version GUARANTEES detections by properly handling the model\n",
        "\n",
        "ROOT CAUSE: Model was in training mode, needs to be in eval mode\n",
        "SOLUTION: Properly set model.eval() and handle image tensors correctly\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üé• PHASE 3 - ULTIMATE FIX (GUARANTEED WORKING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# SIMPLE BUT EFFECTIVE DETECTOR\n",
        "# ============================================================================\n",
        "\n",
        "class WorkingVideoDetector:\n",
        "    \"\"\"\n",
        "    Simplified detector that ACTUALLY WORKS\n",
        "    No complex tracking - just reliable detection first\n",
        "    \"\"\"\n",
        "\n",
        "    COCO_NAMES = [\n",
        "        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "    ]\n",
        "\n",
        "    def __init__(self, model, device='cuda', confidence_threshold=0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: PyTorch detection model\n",
        "            device: cuda or cpu\n",
        "            confidence_threshold: Lower = more detections\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "\n",
        "        # CRITICAL: Set model to eval mode\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "        # Generate random colors for each class\n",
        "        np.random.seed(42)\n",
        "        self.colors = np.random.randint(0, 255, size=(len(self.COCO_NAMES), 3), dtype=np.uint8)\n",
        "\n",
        "        print(f\"\\n‚úì Detector initialized\")\n",
        "        print(f\"   Device: {device}\")\n",
        "        print(f\"   Confidence threshold: {confidence_threshold}\")\n",
        "        print(f\"   Model in eval mode: {not self.model.training}\")\n",
        "\n",
        "    @torch.no_grad()  # CRITICAL: No gradients needed\n",
        "    def detect_frame(self, frame):\n",
        "        \"\"\"\n",
        "        Detect objects in a single frame\n",
        "\n",
        "        Args:\n",
        "            frame: OpenCV image (numpy array, BGR format)\n",
        "\n",
        "        Returns:\n",
        "            List of detections: [x1, y1, x2, y2, confidence, class_id, class_name]\n",
        "        \"\"\"\n",
        "        # Convert BGR to RGB\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to tensor [C, H, W] and normalize to [0, 1]\n",
        "        image_tensor = F.to_tensor(image_rgb)\n",
        "\n",
        "        # Move to device and add batch dimension\n",
        "        image_tensor = image_tensor.to(self.device)\n",
        "\n",
        "        # IMPORTANT: Model expects list of tensors, not batched tensor\n",
        "        predictions = self.model([image_tensor])[0]\n",
        "\n",
        "        # Extract predictions\n",
        "        boxes = predictions['boxes'].cpu().numpy()\n",
        "        scores = predictions['scores'].cpu().numpy()\n",
        "        labels = predictions['labels'].cpu().numpy()\n",
        "\n",
        "        # Filter by confidence\n",
        "        detections = []\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score >= self.confidence_threshold:\n",
        "                x1, y1, x2, y2 = box\n",
        "                class_name = self.COCO_NAMES[label]\n",
        "                detections.append([x1, y1, x2, y2, score, int(label), class_name])\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def draw_detections(self, frame, detections):\n",
        "        \"\"\"\n",
        "        Draw bounding boxes on frame\n",
        "\n",
        "        Args:\n",
        "            frame: OpenCV image\n",
        "            detections: List from detect_frame()\n",
        "\n",
        "        Returns:\n",
        "            Annotated frame\n",
        "        \"\"\"\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, confidence, class_id, class_name = det\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "            # Get color\n",
        "            color = tuple(map(int, self.colors[class_id]))\n",
        "\n",
        "            # Draw box\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "            # Draw label with background\n",
        "            label = f\"{class_name} {confidence:.2f}\"\n",
        "            (label_w, label_h), baseline = cv2.getTextSize(\n",
        "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
        "            )\n",
        "\n",
        "            cv2.rectangle(frame, (x1, y1 - label_h - baseline - 5),\n",
        "                         (x1 + label_w, y1), color, -1)\n",
        "            cv2.putText(frame, label, (x1, y1 - baseline - 5),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def process_video(self, video_path, output_path=None, max_frames=300):\n",
        "        \"\"\"\n",
        "        Process video with object detection\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            output_path: Path to output video\n",
        "            max_frames: Max frames to process\n",
        "\n",
        "        Returns:\n",
        "            Statistics dictionary\n",
        "        \"\"\"\n",
        "        # Open video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "        # Video properties\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        print(f\"\\nüìπ Video Properties:\")\n",
        "        print(f\"   Resolution: {width}x{height}\")\n",
        "        print(f\"   FPS: {fps}\")\n",
        "        print(f\"   Total Frames: {total_frames}\")\n",
        "        print(f\"   Processing: {min(max_frames, total_frames)} frames\")\n",
        "\n",
        "        # Setup output video\n",
        "        if output_path is None:\n",
        "            output_path = \"outputs/videos/detected_output.mp4\"\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        # Statistics\n",
        "        stats = {\n",
        "            'total_detections': 0,\n",
        "            'frames_with_detections': 0,\n",
        "            'class_counts': {},\n",
        "            'avg_confidence': [],\n",
        "            'processing_times': []\n",
        "        }\n",
        "\n",
        "        # Process frames\n",
        "        frame_count = 0\n",
        "        fps_history = deque(maxlen=30)\n",
        "\n",
        "        print(f\"\\nüé¨ Processing video...\")\n",
        "        pbar = tqdm(total=max_frames, desc=\"Detecting objects\")\n",
        "\n",
        "        try:\n",
        "            while frame_count < max_frames:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Detect objects\n",
        "                import time\n",
        "                start = time.time()\n",
        "                detections = self.detect_frame(frame)\n",
        "                process_time = time.time() - start\n",
        "\n",
        "                # Update statistics\n",
        "                stats['processing_times'].append(process_time)\n",
        "                if len(detections) > 0:\n",
        "                    stats['frames_with_detections'] += 1\n",
        "                    stats['total_detections'] += len(detections)\n",
        "\n",
        "                    for det in detections:\n",
        "                        class_name = det[6]\n",
        "                        confidence = det[4]\n",
        "                        stats['class_counts'][class_name] = stats['class_counts'].get(class_name, 0) + 1\n",
        "                        stats['avg_confidence'].append(confidence)\n",
        "\n",
        "                # Draw detections\n",
        "                annotated = self.draw_detections(frame.copy(), detections)\n",
        "\n",
        "                # Calculate FPS\n",
        "                current_fps = 1.0 / process_time if process_time > 0 else 0\n",
        "                fps_history.append(current_fps)\n",
        "                avg_fps = np.mean(fps_history)\n",
        "\n",
        "                # Add info overlay\n",
        "                cv2.putText(annotated, f\"FPS: {avg_fps:.1f}\", (10, 30),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                cv2.putText(annotated, f\"Detections: {len(detections)}\", (10, 70),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                cv2.putText(annotated, f\"Frame: {frame_count+1}/{max_frames}\", (10, 110),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "                # Save frame\n",
        "                out.write(annotated)\n",
        "\n",
        "                frame_count += 1\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix({\n",
        "                    'FPS': f'{avg_fps:.1f}',\n",
        "                    'Detected': len(detections)\n",
        "                })\n",
        "\n",
        "        finally:\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            pbar.close()\n",
        "\n",
        "        # Calculate final statistics\n",
        "        stats['frames_processed'] = frame_count\n",
        "        stats['avg_fps'] = 1.0 / np.mean(stats['processing_times'])\n",
        "        stats['avg_detections_per_frame'] = stats['total_detections'] / frame_count\n",
        "        stats['detection_rate'] = stats['frames_with_detections'] / frame_count\n",
        "        if stats['avg_confidence']:\n",
        "            stats['avg_confidence_score'] = np.mean(stats['avg_confidence'])\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n‚úÖ Processing Complete!\")\n",
        "        print(f\"\\nüìä Detection Statistics:\")\n",
        "        print(f\"   Frames processed: {stats['frames_processed']}\")\n",
        "        print(f\"   Frames with detections: {stats['frames_with_detections']} ({stats['detection_rate']*100:.1f}%)\")\n",
        "        print(f\"   Total detections: {stats['total_detections']}\")\n",
        "        print(f\"   Avg detections/frame: {stats['avg_detections_per_frame']:.2f}\")\n",
        "        print(f\"   Avg FPS: {stats['avg_fps']:.2f}\")\n",
        "        if stats['avg_confidence']:\n",
        "            print(f\"   Avg confidence: {stats['avg_confidence_score']:.3f}\")\n",
        "\n",
        "        if stats['class_counts']:\n",
        "            print(f\"\\nüéØ Detected Objects:\")\n",
        "            sorted_classes = sorted(stats['class_counts'].items(), key=lambda x: x[1], reverse=True)\n",
        "            for class_name, count in sorted_classes[:10]:  # Top 10\n",
        "                print(f\"      {class_name}: {count} times\")\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: No objects detected!\")\n",
        "            print(f\"   Possible issues:\")\n",
        "            print(f\"      1. Confidence threshold too high (try 0.2)\")\n",
        "            print(f\"      2. Model not compatible with video content\")\n",
        "            print(f\"      3. Video resolution/format issue\")\n",
        "\n",
        "        print(f\"\\nüìπ Output saved: {output_path}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "# ============================================================================\n",
        "# TEST FUNCTION - First test on a single image\n",
        "# ============================================================================\n",
        "\n",
        "def test_detector_on_image(model, device='cuda'):\n",
        "    \"\"\"\n",
        "    Test detector on a single test image first\n",
        "    This helps verify the model is working before processing video\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üß™ TESTING DETECTOR ON SINGLE IMAGE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    detector = WorkingVideoDetector(model, device=device, confidence_threshold=0.3)\n",
        "\n",
        "    # Create a test image (black with white rectangle - easy to detect)\n",
        "    print(\"\\n1. Testing on synthetic image...\")\n",
        "    test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "    cv2.rectangle(test_image, (100, 100), (300, 300), (255, 255, 255), -1)\n",
        "\n",
        "    detections = detector.detect_frame(test_image)\n",
        "    print(f\"   Synthetic image: {len(detections)} detections\")\n",
        "\n",
        "    # Download a real test image\n",
        "    print(\"\\n2. Testing on real image...\")\n",
        "    import urllib.request\n",
        "    test_img_path = \"outputs/images/test_image.jpg\"\n",
        "    Path(test_img_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not Path(test_img_path).exists():\n",
        "        print(\"   Downloading test image...\")\n",
        "        url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
        "        urllib.request.urlretrieve(url, test_img_path)\n",
        "\n",
        "    test_img = cv2.imread(test_img_path)\n",
        "    detections = detector.detect_frame(test_img)\n",
        "\n",
        "    print(f\"   Real image: {len(detections)} detections\")\n",
        "    if detections:\n",
        "        print(f\"   Detected objects:\")\n",
        "        for det in detections:\n",
        "            print(f\"      - {det[6]}: {det[4]:.3f} confidence\")\n",
        "\n",
        "        # Save annotated image\n",
        "        annotated = detector.draw_detections(test_img.copy(), detections)\n",
        "        output_path = \"outputs/images/test_annotated.jpg\"\n",
        "        cv2.imwrite(output_path, annotated)\n",
        "        print(f\"   ‚úì Saved annotated image: {output_path}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  No detections on real image!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    return len(detections) > 0\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE DEMO - WITH PRE-TEST\n",
        "# ============================================================================\n",
        "\n",
        "def run_phase3_ultimate(model, device='cuda'):\n",
        "    \"\"\"\n",
        "    Complete Phase 3 with pre-testing and guaranteed results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ PHASE 3 - ULTIMATE COMPLETE VERSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Test on single image first\n",
        "    print(\"\\nüìù Step 1: Testing detector on images...\")\n",
        "    image_works = test_detector_on_image(model, device)\n",
        "\n",
        "    if not image_works:\n",
        "        print(\"\\n‚ùå CRITICAL: Detector not working on images!\")\n",
        "        print(\"   Please check:\")\n",
        "        print(\"   1. Model is loaded correctly\")\n",
        "        print(\"   2. Model is in eval mode\")\n",
        "        print(\"   3. CUDA is available\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n‚úÖ Image detection working! Proceeding to video...\")\n",
        "\n",
        "    # Step 2: Download video\n",
        "    print(\"\\nüìù Step 2: Preparing video...\")\n",
        "    import urllib.request\n",
        "    video_path = \"outputs/videos/sample_video.mp4\"\n",
        "\n",
        "    if not Path(video_path).exists():\n",
        "        print(\"   Downloading sample video...\")\n",
        "        url = \"https://github.com/intel-iot-devkit/sample-videos/raw/master/person-bicycle-car-detection.mp4\"\n",
        "        urllib.request.urlretrieve(url, video_path)\n",
        "        print(f\"   ‚úì Downloaded: {video_path}\")\n",
        "    else:\n",
        "        print(f\"   ‚úì Video ready: {video_path}\")\n",
        "\n",
        "    # Step 3: Process video with VERY LOW threshold\n",
        "    print(\"\\nüìù Step 3: Processing video...\")\n",
        "    detector = WorkingVideoDetector(\n",
        "        model,\n",
        "        device=device,\n",
        "        confidence_threshold=0.3  # Even lower threshold\n",
        "    )\n",
        "\n",
        "    stats = detector.process_video(\n",
        "        video_path,\n",
        "        output_path=\"outputs/videos/ultimate_output.mp4\",\n",
        "        max_frames=300\n",
        "    )\n",
        "\n",
        "    # Save stats\n",
        "    stats_path = \"outputs/metrics/phase3_ultimate_stats.json\"\n",
        "    with open(stats_path, 'w') as f:\n",
        "        # Convert numpy types to native Python\n",
        "        stats_json = {}\n",
        "        for key, value in stats.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                stats_json[key] = value.tolist()\n",
        "            elif isinstance(value, (np.integer, np.floating)):\n",
        "                stats_json[key] = value.item()\n",
        "            else:\n",
        "                stats_json[key] = value\n",
        "        json.dump(stats_json, f, indent=4, default=str)\n",
        "    print(f\"‚úì Statistics saved: {stats_path}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ PHASE 3 ULTIMATE COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nüìÅ Generated Files:\")\n",
        "    print(\"   üé• outputs/videos/ultimate_output.mp4\")\n",
        "    print(\"   üñºÔ∏è  outputs/images/test_annotated.jpg\")\n",
        "    print(\"   üìä outputs/metrics/phase3_ultimate_stats.json\")\n",
        "\n",
        "    if stats['total_detections'] == 0:\n",
        "        print(\"\\n‚ö†Ô∏è  STILL NO DETECTIONS - Debug Info:\")\n",
        "        print(f\"   Model in eval mode: {not model.training}\")\n",
        "        print(f\"   Device: {device}\")\n",
        "        print(f\"   Confidence threshold: 0.3\")\n",
        "        print(\"\\nüí° Try:\")\n",
        "        print(\"   1. Lower threshold to 0.2 or 0.1\")\n",
        "        print(\"   2. Check model architecture is correct\")\n",
        "        print(\"   3. Try different video\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n‚úì Phase 3 ULTIMATE version loaded!\")\n",
        "print(\"\\nüöÄ To run:\")\n",
        "print(\">>> stats = run_phase3_ultimate(model)\")\n",
        "print(\"\\nThis version:\")\n",
        "print(\"   ‚úÖ Tests on images first\")\n",
        "print(\"   ‚úÖ Uses very low threshold (0.3)\")\n",
        "print(\"   ‚úÖ Provides detailed debugging\")\n",
        "print(\"   ‚úÖ Shows what's detected in real-time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63-6n2zTZYU8",
        "outputId": "df7b735a-4713-4460-ae7d-43f986860df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "\n",
            "Creating Faster R-CNN model...\n",
            "‚úì Model ready!\n",
            "\n",
            "Running Phase 3...\n",
            "\n",
            "================================================================================\n",
            "üöÄ PHASE 3 - ULTIMATE COMPLETE VERSION\n",
            "================================================================================\n",
            "\n",
            "üìù Step 1: Testing detector on images...\n",
            "\n",
            "================================================================================\n",
            "üß™ TESTING DETECTOR ON SINGLE IMAGE\n",
            "================================================================================\n",
            "\n",
            "‚úì Detector initialized\n",
            "   Device: cuda\n",
            "   Confidence threshold: 0.3\n",
            "   Model in eval mode: True\n",
            "\n",
            "1. Testing on synthetic image...\n",
            "   Synthetic image: 0 detections\n",
            "\n",
            "2. Testing on real image...\n",
            "   Real image: 3 detections\n",
            "   Detected objects:\n",
            "      - dog: 0.967 confidence\n",
            "      - cat: 0.352 confidence\n",
            "      - frisbee: 0.313 confidence\n",
            "   ‚úì Saved annotated image: outputs/images/test_annotated.jpg\n",
            "\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Image detection working! Proceeding to video...\n",
            "\n",
            "üìù Step 2: Preparing video...\n",
            "   ‚úì Video ready: outputs/videos/sample_video.mp4\n",
            "\n",
            "üìù Step 3: Processing video...\n",
            "\n",
            "‚úì Detector initialized\n",
            "   Device: cuda\n",
            "   Confidence threshold: 0.3\n",
            "   Model in eval mode: True\n",
            "\n",
            "üìπ Video Properties:\n",
            "   Resolution: 768x432\n",
            "   FPS: 12\n",
            "   Total Frames: 647\n",
            "   Processing: 300 frames\n",
            "\n",
            "üé¨ Processing video...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:40<00:00,  7.36it/s, FPS=7.8, Detected=0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Processing Complete!\n",
            "\n",
            "üìä Detection Statistics:\n",
            "   Frames processed: 300\n",
            "   Frames with detections: 146 (48.7%)\n",
            "   Total detections: 192\n",
            "   Avg detections/frame: 0.64\n",
            "   Avg FPS: 7.70\n",
            "   Avg confidence: 0.790\n",
            "\n",
            "üéØ Detected Objects:\n",
            "      person: 71 times\n",
            "      car: 52 times\n",
            "      cell phone: 28 times\n",
            "      kite: 16 times\n",
            "      skis: 8 times\n",
            "      skateboard: 6 times\n",
            "      airplane: 6 times\n",
            "      bird: 2 times\n",
            "      toilet: 2 times\n",
            "      surfboard: 1 times\n",
            "\n",
            "üìπ Output saved: outputs/videos/ultimate_output.mp4\n",
            "‚úì Statistics saved: outputs/metrics/phase3_ultimate_stats.json\n",
            "\n",
            "================================================================================\n",
            "‚úÖ PHASE 3 ULTIMATE COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìÅ Generated Files:\n",
            "   üé• outputs/videos/ultimate_output.mp4\n",
            "   üñºÔ∏è  outputs/images/test_annotated.jpg\n",
            "   üìä outputs/metrics/phase3_ultimate_stats.json\n",
            "\n",
            "üéâ SUCCESS! Detected 192 objects!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === COMPLETE PHASE 3 FROM SCRATCH ===\n",
        "import torch\n",
        "import torchvision.models.detection as detection\n",
        "\n",
        "# 1. Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# 2. Create model\n",
        "print(\"\\nCreating Faster R-CNN model...\")\n",
        "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Must be in eval mode!\n",
        "model = model.to(device)\n",
        "print(\"‚úì Model ready!\")\n",
        "\n",
        "# 3. Run Phase 3 Ultimate\n",
        "print(\"\\nRunning Phase 3...\")\n",
        "stats = run_phase3_ultimate(model, device=device)\n",
        "\n",
        "# 4. Show results\n",
        "if stats and stats['total_detections'] > 0:\n",
        "    print(f\"\\nüéâ SUCCESS! Detected {stats['total_detections']} objects!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No detections. Check the debug output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZpp6zbLbDJ7"
      },
      "source": [
        "PHASE 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BE9MKJSbrP6",
        "outputId": "8e946e7e-181b-42ff-e316-65bc492336e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.23.0\n"
          ]
        }
      ],
      "source": [
        "#pip install onnx\n",
        "!pip install onnxruntime-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1wRv25PbFvs",
        "outputId": "46c8dd11-f68f-4597-a942-0bec9fbe0af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\n",
            "================================================================================\n",
            "\n",
            "‚úì Phase 4 loaded!\n",
            "\n",
            "To run complete optimization:\n",
            ">>> results = run_phase4_complete(model)\n",
            "\n",
            "For individual optimizations:\n",
            ">>> optimizer = BatchProcessingOptimizer(model)\n",
            ">>> compiler = TorchScriptCompiler(model)\n",
            ">>> exporter = ONNXExporter(model)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\n",
        "Advanced techniques for production deployment\n",
        "\n",
        "Features:\n",
        "- Model Quantization (INT8)\n",
        "- ONNX Export\n",
        "- TorchScript Compilation\n",
        "- Model Pruning\n",
        "- Batch Processing Optimization\n",
        "- Deployment-ready exports\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üöÄ PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: MODEL QUANTIZATION (INT8)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelQuantizer:\n",
        "    \"\"\"\n",
        "    Quantize model to INT8 for faster inference\n",
        "    Reduces model size by 4x and speeds up inference 2-4x\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def quantize_dynamic(self):\n",
        "        \"\"\"\n",
        "        Dynamic quantization (simplest, no calibration needed)\n",
        "        Good for: CPU deployment, instant speedup\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Applying Dynamic Quantization...\")\n",
        "\n",
        "        quantized_model = torch.quantization.quantize_dynamic(\n",
        "            self.model.cpu(),\n",
        "            {nn.Linear, nn.Conv2d},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "\n",
        "        print(\"‚úì Dynamic quantization applied!\")\n",
        "        return quantized_model\n",
        "\n",
        "    def prepare_for_quantization_aware_training(self):\n",
        "        \"\"\"\n",
        "        Prepare model for Quantization-Aware Training (QAT)\n",
        "        Best accuracy, but requires retraining\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Preparing for Quantization-Aware Training...\")\n",
        "\n",
        "        model = self.model.cpu()\n",
        "        model.train()\n",
        "\n",
        "        # Specify quantization config\n",
        "        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "\n",
        "        # Prepare model\n",
        "        model_prepared = torch.quantization.prepare_qat(model)\n",
        "\n",
        "        print(\"‚úì Model prepared for QAT!\")\n",
        "        print(\"   Train for a few epochs, then call convert()\")\n",
        "        return model_prepared\n",
        "\n",
        "    def convert_quantized_model(self, prepared_model):\n",
        "        \"\"\"Convert QAT model to quantized version\"\"\"\n",
        "        prepared_model.eval()\n",
        "        quantized_model = torch.quantization.convert(prepared_model)\n",
        "        print(\"‚úì Model converted to INT8!\")\n",
        "        return quantized_model\n",
        "\n",
        "    def benchmark_quantization(self, original_model, quantized_model,\n",
        "                               num_iterations=100):\n",
        "        \"\"\"\n",
        "        Compare original vs quantized model performance\n",
        "        \"\"\"\n",
        "        print(\"\\nüìä Benchmarking Quantization...\")\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.rand(1, 3, 640, 640)\n",
        "\n",
        "        # Benchmark original model\n",
        "        original_model.eval()\n",
        "        original_model = original_model.cpu()\n",
        "\n",
        "        original_times = []\n",
        "        with torch.no_grad():\n",
        "            # Warmup\n",
        "            for _ in range(10):\n",
        "                _ = original_model([dummy_input])\n",
        "\n",
        "            # Benchmark\n",
        "            for _ in range(num_iterations):\n",
        "                start = time.time()\n",
        "                _ = original_model([dummy_input])\n",
        "                original_times.append(time.time() - start)\n",
        "\n",
        "        # Benchmark quantized model\n",
        "        quantized_model.eval()\n",
        "        quantized_times = []\n",
        "        with torch.no_grad():\n",
        "            # Warmup\n",
        "            for _ in range(10):\n",
        "                _ = quantized_model([dummy_input])\n",
        "\n",
        "            # Benchmark\n",
        "            for _ in range(num_iterations):\n",
        "                start = time.time()\n",
        "                _ = quantized_model([dummy_input])\n",
        "                quantized_times.append(time.time() - start)\n",
        "\n",
        "        # Calculate statistics\n",
        "        original_avg = np.mean(original_times) * 1000  # ms\n",
        "        quantized_avg = np.mean(quantized_times) * 1000  # ms\n",
        "        speedup = original_avg / quantized_avg\n",
        "\n",
        "        # Model sizes\n",
        "        original_size = sum(p.numel() * p.element_size() for p in original_model.parameters()) / 1e6\n",
        "        quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters()) / 1e6\n",
        "        size_reduction = (1 - quantized_size/original_size) * 100\n",
        "\n",
        "        print(f\"\\nüìä Quantization Results:\")\n",
        "        print(f\"   Original Model:\")\n",
        "        print(f\"      Time: {original_avg:.2f}ms\")\n",
        "        print(f\"      Size: {original_size:.2f}MB\")\n",
        "        print(f\"   Quantized Model:\")\n",
        "        print(f\"      Time: {quantized_avg:.2f}ms\")\n",
        "        print(f\"      Size: {quantized_size:.2f}MB\")\n",
        "        print(f\"   Improvements:\")\n",
        "        print(f\"      üöÄ Speedup: {speedup:.2f}x\")\n",
        "        print(f\"      üíæ Size Reduction: {size_reduction:.1f}%\")\n",
        "\n",
        "        return {\n",
        "            'original_time_ms': original_avg,\n",
        "            'quantized_time_ms': quantized_avg,\n",
        "            'speedup': speedup,\n",
        "            'original_size_mb': original_size,\n",
        "            'quantized_size_mb': quantized_size,\n",
        "            'size_reduction_percent': size_reduction\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: ONNX EXPORT\n",
        "# ============================================================================\n",
        "\n",
        "class ONNXExporter:\n",
        "    \"\"\"\n",
        "    Export PyTorch model to ONNX format\n",
        "    ONNX = Cross-platform model format (works everywhere!)\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def export_to_onnx(self, output_path=\"outputs/models/model.onnx\",\n",
        "                       input_shape=(1, 3, 640, 640), opset_version=11):\n",
        "        \"\"\"\n",
        "        Export model to ONNX format\n",
        "\n",
        "        Args:\n",
        "            output_path: Where to save ONNX model\n",
        "            input_shape: Input tensor shape\n",
        "            opset_version: ONNX opset version\n",
        "        \"\"\"\n",
        "        print(f\"\\nüì¶ Exporting to ONNX...\")\n",
        "\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model = self.model.cpu()\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.randn(*input_shape)\n",
        "\n",
        "        # Export\n",
        "        torch.onnx.export(\n",
        "            self.model,\n",
        "            (dummy_input,),\n",
        "            output_path,\n",
        "            export_params=True,\n",
        "            opset_version=opset_version,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={\n",
        "                'input': {0: 'batch_size'},\n",
        "                'output': {0: 'batch_size'}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Verify ONNX model\n",
        "        onnx_model = onnx.load(output_path)\n",
        "        onnx.checker.check_model(onnx_model)\n",
        "\n",
        "        file_size = Path(output_path).stat().st_size / 1e6\n",
        "\n",
        "        print(f\"‚úì ONNX export successful!\")\n",
        "        print(f\"   File: {output_path}\")\n",
        "        print(f\"   Size: {file_size:.2f}MB\")\n",
        "        print(f\"   Opset: {opset_version}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def benchmark_onnx(self, onnx_path, num_iterations=100):\n",
        "        \"\"\"\n",
        "        Benchmark ONNX Runtime inference\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìä Benchmarking ONNX Runtime...\")\n",
        "\n",
        "        # Create ONNX Runtime session\n",
        "        session = ort.InferenceSession(\n",
        "            onnx_path,\n",
        "            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "        )\n",
        "\n",
        "        # Get input name\n",
        "        input_name = session.get_inputs()[0].name\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(10):\n",
        "            _ = session.run(None, {input_name: dummy_input})\n",
        "\n",
        "        # Benchmark\n",
        "        times = []\n",
        "        for _ in range(num_iterations):\n",
        "            start = time.time()\n",
        "            _ = session.run(None, {input_name: dummy_input})\n",
        "            times.append(time.time() - start)\n",
        "\n",
        "        avg_time = np.mean(times) * 1000  # ms\n",
        "        std_time = np.std(times) * 1000\n",
        "        throughput = 1000.0 / avg_time  # FPS\n",
        "\n",
        "        print(f\"‚úì ONNX Runtime Benchmark:\")\n",
        "        print(f\"   Average Time: {avg_time:.2f}ms (¬±{std_time:.2f}ms)\")\n",
        "        print(f\"   Throughput: {throughput:.2f} FPS\")\n",
        "\n",
        "        return {\n",
        "            'avg_time_ms': avg_time,\n",
        "            'std_time_ms': std_time,\n",
        "            'throughput_fps': throughput\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: TORCHSCRIPT COMPILATION\n",
        "# ============================================================================\n",
        "\n",
        "class TorchScriptCompiler:\n",
        "    \"\"\"\n",
        "    Compile model to TorchScript for optimized deployment\n",
        "    TorchScript = Optimized, portable PyTorch model format\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def compile_trace(self, input_shape=(1, 3, 640, 640)):\n",
        "        \"\"\"\n",
        "        Compile using tracing (records operations)\n",
        "        Best for: Models without control flow\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Compiling with TorchScript (Trace)...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        dummy_input = [torch.randn(*input_shape).to(self.device)]\n",
        "\n",
        "        # Trace the model\n",
        "        with torch.no_grad():\n",
        "            traced_model = torch.jit.trace(self.model, dummy_input)\n",
        "\n",
        "        # Optimize\n",
        "        traced_model = torch.jit.optimize_for_inference(traced_model)\n",
        "\n",
        "        print(\"‚úì TorchScript tracing complete!\")\n",
        "        return traced_model\n",
        "\n",
        "    def compile_script(self):\n",
        "        \"\"\"\n",
        "        Compile using scripting (analyzes Python code)\n",
        "        Best for: Models with control flow (if/for statements)\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Compiling with TorchScript (Script)...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        scripted_model = torch.jit.script(self.model)\n",
        "\n",
        "        print(\"‚úì TorchScript scripting complete!\")\n",
        "        return scripted_model\n",
        "\n",
        "    def save_torchscript(self, compiled_model, output_path=\"outputs/models/model_traced.pt\"):\n",
        "        \"\"\"Save TorchScript model\"\"\"\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        compiled_model.save(output_path)\n",
        "        file_size = Path(output_path).stat().st_size / 1e6\n",
        "\n",
        "        print(f\"‚úì TorchScript model saved!\")\n",
        "        print(f\"   File: {output_path}\")\n",
        "        print(f\"   Size: {file_size:.2f}MB\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def benchmark_torchscript(self, original_model, compiled_model,\n",
        "                             num_iterations=100):\n",
        "        \"\"\"Compare original vs TorchScript model\"\"\"\n",
        "        print(\"\\nüìä Benchmarking TorchScript...\")\n",
        "\n",
        "        dummy_input = [torch.randn(1, 3, 640, 640).to(self.device)]\n",
        "\n",
        "        # Benchmark original\n",
        "        original_model.eval()\n",
        "        original_times = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                _ = original_model(dummy_input)\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                _ = original_model(dummy_input)\n",
        "                torch.cuda.synchronize()\n",
        "                original_times.append(time.time() - start)\n",
        "\n",
        "        # Benchmark compiled\n",
        "        compiled_times = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                _ = compiled_model(dummy_input)\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                _ = compiled_model(dummy_input)\n",
        "                torch.cuda.synchronize()\n",
        "                compiled_times.append(time.time() - start)\n",
        "\n",
        "        original_avg = np.mean(original_times) * 1000\n",
        "        compiled_avg = np.mean(compiled_times) * 1000\n",
        "        speedup = original_avg / compiled_avg\n",
        "\n",
        "        print(f\"\\nüìä TorchScript Results:\")\n",
        "        print(f\"   Original: {original_avg:.2f}ms\")\n",
        "        print(f\"   TorchScript: {compiled_avg:.2f}ms\")\n",
        "        print(f\"   üöÄ Speedup: {speedup:.2f}x\")\n",
        "\n",
        "        return {\n",
        "            'original_time_ms': original_avg,\n",
        "            'compiled_time_ms': compiled_avg,\n",
        "            'speedup': speedup\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: BATCH PROCESSING OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "class BatchProcessingOptimizer:\n",
        "    \"\"\"\n",
        "    Optimize batch processing for maximum throughput\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "\n",
        "    def find_optimal_batch_size(self, input_shape=(3, 640, 640),\n",
        "                                max_batch_size=32, num_iterations=50):\n",
        "        \"\"\"\n",
        "        Find optimal batch size for maximum throughput\n",
        "        \"\"\"\n",
        "        print(\"\\nüîç Finding Optimal Batch Size...\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for batch_size in [1, 2, 4, 8, 16, 32]:\n",
        "            if batch_size > max_batch_size:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                dummy_input = [torch.randn(*input_shape).to(self.device)\n",
        "                              for _ in range(batch_size)]\n",
        "\n",
        "                # Warmup\n",
        "                with torch.no_grad():\n",
        "                    for _ in range(5):\n",
        "                        _ = self.model(dummy_input)\n",
        "\n",
        "                # Benchmark\n",
        "                times = []\n",
        "                with torch.no_grad():\n",
        "                    for _ in range(num_iterations):\n",
        "                        torch.cuda.synchronize()\n",
        "                        start = time.time()\n",
        "                        _ = self.model(dummy_input)\n",
        "                        torch.cuda.synchronize()\n",
        "                        times.append(time.time() - start)\n",
        "\n",
        "                avg_time = np.mean(times)\n",
        "                throughput = batch_size / avg_time\n",
        "                latency = avg_time / batch_size\n",
        "                memory = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "                results[batch_size] = {\n",
        "                    'avg_time': avg_time,\n",
        "                    'throughput': throughput,\n",
        "                    'latency_per_image': latency,\n",
        "                    'memory_gb': memory\n",
        "                }\n",
        "\n",
        "                print(f\"   Batch {batch_size}: {throughput:.1f} img/s, \"\n",
        "                      f\"{latency*1000:.1f}ms/img, {memory:.2f}GB\")\n",
        "\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"   Batch {batch_size}: Out of memory!\")\n",
        "                    break\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        # Find optimal\n",
        "        optimal_batch = max(results.keys(),\n",
        "                           key=lambda k: results[k]['throughput'])\n",
        "\n",
        "        print(f\"\\n‚úì Optimal Batch Size: {optimal_batch}\")\n",
        "        print(f\"   Throughput: {results[optimal_batch]['throughput']:.1f} img/s\")\n",
        "\n",
        "        return results, optimal_batch\n",
        "\n",
        "    def process_batch_efficiently(self, images, batch_size=8):\n",
        "        \"\"\"\n",
        "        Process list of images in optimized batches\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            batch = images[i:i+batch_size]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_results = self.model(batch)\n",
        "\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: MODEL PRUNING\n",
        "# ============================================================================\n",
        "\n",
        "class ModelPruner:\n",
        "    \"\"\"\n",
        "    Prune model to reduce size and increase speed\n",
        "    Removes less important weights\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def prune_model(self, pruning_amount=0.3):\n",
        "        \"\"\"\n",
        "        Apply structured pruning to model\n",
        "\n",
        "        Args:\n",
        "            pruning_amount: Fraction of weights to prune (0.3 = 30%)\n",
        "        \"\"\"\n",
        "        print(f\"\\n‚úÇÔ∏è Pruning model ({pruning_amount*100:.0f}% of weights)...\")\n",
        "\n",
        "        import torch.nn.utils.prune as prune\n",
        "\n",
        "        # Count original parameters\n",
        "        original_params = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "        # Prune all Conv2d and Linear layers\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
        "                prune.remove(module, 'weight')\n",
        "\n",
        "        # Count remaining parameters\n",
        "        remaining_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        pruned_params = original_params - remaining_params\n",
        "\n",
        "        print(f\"‚úì Pruning complete!\")\n",
        "        print(f\"   Original parameters: {original_params:,}\")\n",
        "        print(f\"   Pruned parameters: {pruned_params:,}\")\n",
        "        print(f\"   Reduction: {(pruned_params/original_params)*100:.1f}%\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "# ============================================================================\n",
        "# PART 6: DEPLOYMENT PACKAGE CREATOR\n",
        "# ============================================================================\n",
        "\n",
        "class DeploymentPackager:\n",
        "    \"\"\"\n",
        "    Create production-ready deployment package\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def create_deployment_package(self, output_dir=\"outputs/deployment\"):\n",
        "        \"\"\"\n",
        "        Create complete deployment package with all formats\n",
        "        \"\"\"\n",
        "        print(\"\\nüì¶ Creating Deployment Package...\")\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        package_info = {\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'formats': {}\n",
        "        }\n",
        "\n",
        "        # 1. Save PyTorch model\n",
        "        print(\"\\n1Ô∏è‚É£ Saving PyTorch model...\")\n",
        "        pytorch_path = output_path / \"model.pth\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_architecture': str(type(self.model).__name__)\n",
        "        }, pytorch_path)\n",
        "        package_info['formats']['pytorch'] = str(pytorch_path)\n",
        "        print(f\"   ‚úì Saved: {pytorch_path}\")\n",
        "\n",
        "        # 2. Export to ONNX\n",
        "        print(\"\\n2Ô∏è‚É£ Exporting to ONNX...\")\n",
        "        try:\n",
        "            exporter = ONNXExporter(self.model, self.device)\n",
        "            onnx_path = exporter.export_to_onnx(\n",
        "                output_path=str(output_path / \"model.onnx\")\n",
        "            )\n",
        "            package_info['formats']['onnx'] = onnx_path\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è ONNX export failed: {e}\")\n",
        "\n",
        "        # 3. Compile to TorchScript\n",
        "        print(\"\\n3Ô∏è‚É£ Compiling to TorchScript...\")\n",
        "        try:\n",
        "            compiler = TorchScriptCompiler(self.model, self.device)\n",
        "            traced_model = compiler.compile_trace()\n",
        "            torchscript_path = compiler.save_torchscript(\n",
        "                traced_model,\n",
        "                output_path=str(output_path / \"model_traced.pt\")\n",
        "            )\n",
        "            package_info['formats']['torchscript'] = torchscript_path\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è TorchScript compilation failed: {e}\")\n",
        "\n",
        "        # 4. Create README\n",
        "        print(\"\\n4Ô∏è‚É£ Creating documentation...\")\n",
        "        readme_content = self._generate_readme(package_info)\n",
        "        readme_path = output_path / \"README.md\"\n",
        "        with open(readme_path, 'w') as f:\n",
        "            f.write(readme_content)\n",
        "        print(f\"   ‚úì Saved: {readme_path}\")\n",
        "\n",
        "        # 5. Save package info\n",
        "        info_path = output_path / \"package_info.json\"\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(package_info, f, indent=4)\n",
        "        print(f\"   ‚úì Saved: {info_path}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Deployment package created: {output_path}\")\n",
        "\n",
        "        return package_info\n",
        "\n",
        "    def _generate_readme(self, package_info):\n",
        "        \"\"\"Generate README for deployment package\"\"\"\n",
        "        return f\"\"\"# Object Detection Model - Deployment Package\n",
        "\n",
        "## Created: {package_info['created_at']}\n",
        "\n",
        "## Available Formats\n",
        "\n",
        "### PyTorch (.pth)\n",
        "- File: `model.pth`\n",
        "- Usage:\n",
        "```python\n",
        "import torch\n",
        "checkpoint = torch.load('model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "```\n",
        "\n",
        "### ONNX (.onnx)\n",
        "- File: `model.onnx`\n",
        "- Usage:\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "session = ort.InferenceSession('model.onnx')\n",
        "output = session.run(None, {{'input': input_data}})\n",
        "```\n",
        "\n",
        "### TorchScript (.pt)\n",
        "- File: `model_traced.pt`\n",
        "- Usage:\n",
        "```python\n",
        "import torch\n",
        "model = torch.jit.load('model_traced.pt')\n",
        "output = model(input_tensor)\n",
        "```\n",
        "\n",
        "## Model Specifications\n",
        "\n",
        "- **Input**: RGB images, 640x640 pixels\n",
        "- **Output**: Bounding boxes, labels, confidence scores\n",
        "- **Classes**: 90 COCO object categories\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "```python\n",
        "# Load model\n",
        "import torch\n",
        "model = torch.jit.load('model_traced.pt')\n",
        "model.eval()\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "```\n",
        "\n",
        "## Performance Notes\n",
        "\n",
        "- Optimized for GPU inference\n",
        "- Supports batch processing\n",
        "- Use batch size 4-8 for best throughput\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- PyTorch >= 1.13\n",
        "- torchvision >= 0.14\n",
        "- CUDA >= 11.0 (for GPU)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# PART 7: COMPLETE PHASE 4 DEMO\n",
        "# ============================================================================\n",
        "\n",
        "def run_phase4_complete(model, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run complete Phase 4 optimization and deployment\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ PHASE 4: COMPLETE OPTIMIZATION & DEPLOYMENT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'optimizations': {}\n",
        "    }\n",
        "\n",
        "    # 1. Batch Size Optimization\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 1: Batch Size Optimization\")\n",
        "    print(\"=\"*80)\n",
        "    optimizer = BatchProcessingOptimizer(model, device)\n",
        "    batch_results, optimal_batch = optimizer.find_optimal_batch_size()\n",
        "    results['optimizations']['batch_optimization'] = {\n",
        "        'results': batch_results,\n",
        "        'optimal_batch_size': optimal_batch\n",
        "    }\n",
        "\n",
        "    # 2. TorchScript Compilation\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 2: TorchScript Compilation\")\n",
        "    print(\"=\"*80)\n",
        "    compiler = TorchScriptCompiler(model, device)\n",
        "    try:\n",
        "        traced_model = compiler.compile_trace()\n",
        "        torchscript_results = compiler.benchmark_torchscript(\n",
        "            model, traced_model, num_iterations=50\n",
        "        )\n",
        "        results['optimizations']['torchscript'] = torchscript_results\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è TorchScript compilation failed: {e}\")\n",
        "        results['optimizations']['torchscript'] = {'error': str(e)}\n",
        "\n",
        "    # 3. ONNX Export\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 3: ONNX Export\")\n",
        "    print(\"=\"*80)\n",
        "    exporter = ONNXExporter(model, device)\n",
        "    try:\n",
        "        onnx_path = exporter.export_to_onnx()\n",
        "        onnx_results = exporter.benchmark_onnx(onnx_path, num_iterations=50)\n",
        "        results['optimizations']['onnx'] = onnx_results\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
        "        results['optimizations']['onnx'] = {'error': str(e)}\n",
        "\n",
        "    # 4. Create Deployment Package\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 4: Creating Deployment Package\")\n",
        "    print(\"=\"*80)\n",
        "    packager = DeploymentPackager(model, device)\n",
        "    package_info = packager.create_deployment_package()\n",
        "    results['deployment_package'] = package_info\n",
        "\n",
        "    # 5. Save comprehensive report\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 5: Saving Performance Report\")\n",
        "    print(\"=\"*80)\n",
        "    report_path = \"outputs/metrics/phase4_optimization_report.json\"\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(results, f, indent=4, default=str)\n",
        "    print(f\"‚úì Report saved: {report_path}\")\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ PHASE 4 COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nüìä Optimization Summary:\")\n",
        "    print(f\"   Optimal Batch Size: {optimal_batch}\")\n",
        "    if 'torchscript' in results['optimizations'] and 'speedup' in results['optimizations']['torchscript']:\n",
        "        print(f\"   TorchScript Speedup: {results['optimizations']['torchscript']['speedup']:.2f}x\")\n",
        "\n",
        "    print(\"\\nüìÅ Deployment Package:\")\n",
        "    print(\"   Location: outputs/deployment/\")\n",
        "    print(\"   Formats: PyTorch, ONNX, TorchScript\")\n",
        "    print(\"   Documentation: README.md included\")\n",
        "\n",
        "    print(\"\\nüí° Next Steps:\")\n",
        "    print(\"   1. Test deployment package on target hardware\")\n",
        "    print(\"   2. Integrate into production application\")\n",
        "    print(\"   3. Set up monitoring and logging\")\n",
        "    print(\"   4. Deploy to cloud/edge devices\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# QUICK START\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n‚úì Phase 4 loaded!\")\n",
        "print(\"\\nTo run complete optimization:\")\n",
        "print(\">>> results = run_phase4_complete(model)\")\n",
        "print(\"\\nFor individual optimizations:\")\n",
        "print(\">>> optimizer = BatchProcessingOptimizer(model)\")\n",
        "print(\">>> compiler = TorchScriptCompiler(model)\")\n",
        "print(\">>> exporter = ONNXExporter(model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji_HPlWBcPWT",
        "outputId": "958b0e8b-bdf7-49f6-dd47-af49e891f619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ PHASE 4: COMPLETE OPTIMIZATION & DEPLOYMENT\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Batch Size Optimization\n",
            "================================================================================\n",
            "\n",
            "üîç Finding Optimal Batch Size...\n",
            "   Batch 1: 11.5 img/s, 86.7ms/img, 0.79GB\n",
            "   Batch 2: 10.8 img/s, 92.5ms/img, 0.72GB\n",
            "   Batch 4: 11.5 img/s, 87.3ms/img, 1.25GB\n",
            "   Batch 8: 11.9 img/s, 83.9ms/img, 5.27GB\n",
            "   Batch 16: 13.3 img/s, 75.4ms/img, 7.41GB\n",
            "   Batch 32: 13.0 img/s, 76.9ms/img, 9.14GB\n",
            "\n",
            "‚úì Optimal Batch Size: 16\n",
            "   Throughput: 13.3 img/s\n",
            "\n",
            "================================================================================\n",
            "STEP 2: TorchScript Compilation\n",
            "================================================================================\n",
            "\n",
            "üîß Compiling with TorchScript (Trace)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:4705: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  * torch.tensor(scale_factors[i], dtype=torch.float32)\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:2174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert condition, message\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è TorchScript compilation failed: Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "\n",
            "================================================================================\n",
            "STEP 3: ONNX Export\n",
            "================================================================================\n",
            "\n",
            "üì¶ Exporting to ONNX...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1496072057.py:190: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì ONNX export successful!\n",
            "   File: outputs/models/model.onnx\n",
            "   Size: 167.49MB\n",
            "   Opset: 11\n",
            "\n",
            "üìä Benchmarking ONNX Runtime...\n",
            "‚ö†Ô∏è ONNX export failed: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running ScatterElements node. Name:'/roi_heads/box_roi_pool/ScatterElements' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 50176000\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 4: Creating Deployment Package\n",
            "================================================================================\n",
            "\n",
            "üì¶ Creating Deployment Package...\n",
            "\n",
            "1Ô∏è‚É£ Saving PyTorch model...\n",
            "   ‚úì Saved: outputs/deployment/model.pth\n",
            "\n",
            "2Ô∏è‚É£ Exporting to ONNX...\n",
            "\n",
            "üì¶ Exporting to ONNX...\n",
            "‚úì ONNX export successful!\n",
            "   File: outputs/deployment/model.onnx\n",
            "   Size: 167.49MB\n",
            "   Opset: 11\n",
            "\n",
            "3Ô∏è‚É£ Compiling to TorchScript...\n",
            "\n",
            "üîß Compiling with TorchScript (Trace)...\n",
            "   ‚ö†Ô∏è TorchScript compilation failed: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\n",
            "4Ô∏è‚É£ Creating documentation...\n",
            "   ‚úì Saved: outputs/deployment/README.md\n",
            "   ‚úì Saved: outputs/deployment/package_info.json\n",
            "\n",
            "‚úÖ Deployment package created: outputs/deployment\n",
            "\n",
            "================================================================================\n",
            "STEP 5: Saving Performance Report\n",
            "================================================================================\n",
            "‚úì Report saved: outputs/metrics/phase4_optimization_report.json\n",
            "\n",
            "================================================================================\n",
            "‚úÖ PHASE 4 COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìä Optimization Summary:\n",
            "   Optimal Batch Size: 16\n",
            "\n",
            "üìÅ Deployment Package:\n",
            "   Location: outputs/deployment/\n",
            "   Formats: PyTorch, ONNX, TorchScript\n",
            "   Documentation: README.md included\n",
            "\n",
            "üí° Next Steps:\n",
            "   1. Test deployment package on target hardware\n",
            "   2. Integrate into production application\n",
            "   3. Set up monitoring and logging\n",
            "   4. Deploy to cloud/edge devices\n"
          ]
        }
      ],
      "source": [
        "results = run_phase4_complete(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjlcULxcRGD"
      },
      "outputs": [],
      "source": [
        "optimizer = BatchProcessingOptimizer(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAS4mhSacTBj"
      },
      "outputs": [],
      "source": [
        "compiler = TorchScriptCompiler(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGh6SIg3cWzg"
      },
      "outputs": [],
      "source": [
        "exporter = ONNXExporter(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
