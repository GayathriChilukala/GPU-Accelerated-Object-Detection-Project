{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZpp6zbLbDJ7"
      },
      "source": [
        "PHASE 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BE9MKJSbrP6",
        "outputId": "8e946e7e-181b-42ff-e316-65bc492336e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.23.0\n"
          ]
        }
      ],
      "source": [
        "#pip install onnx\n",
        "!pip install onnxruntime-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1wRv25PbFvs",
        "outputId": "46c8dd11-f68f-4597-a942-0bec9fbe0af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🚀 PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\n",
            "================================================================================\n",
            "\n",
            "✓ Phase 4 loaded!\n",
            "\n",
            "To run complete optimization:\n",
            ">>> results = run_phase4_complete(model)\n",
            "\n",
            "For individual optimizations:\n",
            ">>> optimizer = BatchProcessingOptimizer(model)\n",
            ">>> compiler = TorchScriptCompiler(model)\n",
            ">>> exporter = ONNXExporter(model)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\n",
        "Advanced techniques for production deployment\n",
        "\n",
        "Features:\n",
        "- Model Quantization (INT8)\n",
        "- ONNX Export\n",
        "- TorchScript Compilation\n",
        "- Model Pruning\n",
        "- Batch Processing Optimization\n",
        "- Deployment-ready exports\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🚀 PHASE 4: MODEL OPTIMIZATION & DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: MODEL QUANTIZATION (INT8)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelQuantizer:\n",
        "    \"\"\"\n",
        "    Quantize model to INT8 for faster inference\n",
        "    Reduces model size by 4x and speeds up inference 2-4x\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def quantize_dynamic(self):\n",
        "        \"\"\"\n",
        "        Dynamic quantization (simplest, no calibration needed)\n",
        "        Good for: CPU deployment, instant speedup\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 Applying Dynamic Quantization...\")\n",
        "\n",
        "        quantized_model = torch.quantization.quantize_dynamic(\n",
        "            self.model.cpu(),\n",
        "            {nn.Linear, nn.Conv2d},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "\n",
        "        print(\"✓ Dynamic quantization applied!\")\n",
        "        return quantized_model\n",
        "\n",
        "    def prepare_for_quantization_aware_training(self):\n",
        "        \"\"\"\n",
        "        Prepare model for Quantization-Aware Training (QAT)\n",
        "        Best accuracy, but requires retraining\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 Preparing for Quantization-Aware Training...\")\n",
        "\n",
        "        model = self.model.cpu()\n",
        "        model.train()\n",
        "\n",
        "        # Specify quantization config\n",
        "        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "\n",
        "        # Prepare model\n",
        "        model_prepared = torch.quantization.prepare_qat(model)\n",
        "\n",
        "        print(\"✓ Model prepared for QAT!\")\n",
        "        print(\"   Train for a few epochs, then call convert()\")\n",
        "        return model_prepared\n",
        "\n",
        "    def convert_quantized_model(self, prepared_model):\n",
        "        \"\"\"Convert QAT model to quantized version\"\"\"\n",
        "        prepared_model.eval()\n",
        "        quantized_model = torch.quantization.convert(prepared_model)\n",
        "        print(\"✓ Model converted to INT8!\")\n",
        "        return quantized_model\n",
        "\n",
        "    def benchmark_quantization(self, original_model, quantized_model,\n",
        "                               num_iterations=100):\n",
        "        \"\"\"\n",
        "        Compare original vs quantized model performance\n",
        "        \"\"\"\n",
        "        print(\"\\n📊 Benchmarking Quantization...\")\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.rand(1, 3, 640, 640)\n",
        "\n",
        "        # Benchmark original model\n",
        "        original_model.eval()\n",
        "        original_model = original_model.cpu()\n",
        "\n",
        "        original_times = []\n",
        "        with torch.no_grad():\n",
        "            # Warmup\n",
        "            for _ in range(10):\n",
        "                _ = original_model([dummy_input])\n",
        "\n",
        "            # Benchmark\n",
        "            for _ in range(num_iterations):\n",
        "                start = time.time()\n",
        "                _ = original_model([dummy_input])\n",
        "                original_times.append(time.time() - start)\n",
        "\n",
        "        # Benchmark quantized model\n",
        "        quantized_model.eval()\n",
        "        quantized_times = []\n",
        "        with torch.no_grad():\n",
        "            # Warmup\n",
        "            for _ in range(10):\n",
        "                _ = quantized_model([dummy_input])\n",
        "\n",
        "            # Benchmark\n",
        "            for _ in range(num_iterations):\n",
        "                start = time.time()\n",
        "                _ = quantized_model([dummy_input])\n",
        "                quantized_times.append(time.time() - start)\n",
        "\n",
        "        # Calculate statistics\n",
        "        original_avg = np.mean(original_times) * 1000  # ms\n",
        "        quantized_avg = np.mean(quantized_times) * 1000  # ms\n",
        "        speedup = original_avg / quantized_avg\n",
        "\n",
        "        # Model sizes\n",
        "        original_size = sum(p.numel() * p.element_size() for p in original_model.parameters()) / 1e6\n",
        "        quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters()) / 1e6\n",
        "        size_reduction = (1 - quantized_size/original_size) * 100\n",
        "\n",
        "        print(f\"\\n📊 Quantization Results:\")\n",
        "        print(f\"   Original Model:\")\n",
        "        print(f\"      Time: {original_avg:.2f}ms\")\n",
        "        print(f\"      Size: {original_size:.2f}MB\")\n",
        "        print(f\"   Quantized Model:\")\n",
        "        print(f\"      Time: {quantized_avg:.2f}ms\")\n",
        "        print(f\"      Size: {quantized_size:.2f}MB\")\n",
        "        print(f\"   Improvements:\")\n",
        "        print(f\"      🚀 Speedup: {speedup:.2f}x\")\n",
        "        print(f\"      💾 Size Reduction: {size_reduction:.1f}%\")\n",
        "\n",
        "        return {\n",
        "            'original_time_ms': original_avg,\n",
        "            'quantized_time_ms': quantized_avg,\n",
        "            'speedup': speedup,\n",
        "            'original_size_mb': original_size,\n",
        "            'quantized_size_mb': quantized_size,\n",
        "            'size_reduction_percent': size_reduction\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: ONNX EXPORT\n",
        "# ============================================================================\n",
        "\n",
        "class ONNXExporter:\n",
        "    \"\"\"\n",
        "    Export PyTorch model to ONNX format\n",
        "    ONNX = Cross-platform model format (works everywhere!)\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def export_to_onnx(self, output_path=\"outputs/models/model.onnx\",\n",
        "                       input_shape=(1, 3, 640, 640), opset_version=11):\n",
        "        \"\"\"\n",
        "        Export model to ONNX format\n",
        "\n",
        "        Args:\n",
        "            output_path: Where to save ONNX model\n",
        "            input_shape: Input tensor shape\n",
        "            opset_version: ONNX opset version\n",
        "        \"\"\"\n",
        "        print(f\"\\n📦 Exporting to ONNX...\")\n",
        "\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model = self.model.cpu()\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.randn(*input_shape)\n",
        "\n",
        "        # Export\n",
        "        torch.onnx.export(\n",
        "            self.model,\n",
        "            (dummy_input,),\n",
        "            output_path,\n",
        "            export_params=True,\n",
        "            opset_version=opset_version,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={\n",
        "                'input': {0: 'batch_size'},\n",
        "                'output': {0: 'batch_size'}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Verify ONNX model\n",
        "        onnx_model = onnx.load(output_path)\n",
        "        onnx.checker.check_model(onnx_model)\n",
        "\n",
        "        file_size = Path(output_path).stat().st_size / 1e6\n",
        "\n",
        "        print(f\"✓ ONNX export successful!\")\n",
        "        print(f\"   File: {output_path}\")\n",
        "        print(f\"   Size: {file_size:.2f}MB\")\n",
        "        print(f\"   Opset: {opset_version}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def benchmark_onnx(self, onnx_path, num_iterations=100):\n",
        "        \"\"\"\n",
        "        Benchmark ONNX Runtime inference\n",
        "        \"\"\"\n",
        "        print(f\"\\n📊 Benchmarking ONNX Runtime...\")\n",
        "\n",
        "        # Create ONNX Runtime session\n",
        "        session = ort.InferenceSession(\n",
        "            onnx_path,\n",
        "            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "        )\n",
        "\n",
        "        # Get input name\n",
        "        input_name = session.get_inputs()[0].name\n",
        "\n",
        "        # Create dummy input\n",
        "        dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(10):\n",
        "            _ = session.run(None, {input_name: dummy_input})\n",
        "\n",
        "        # Benchmark\n",
        "        times = []\n",
        "        for _ in range(num_iterations):\n",
        "            start = time.time()\n",
        "            _ = session.run(None, {input_name: dummy_input})\n",
        "            times.append(time.time() - start)\n",
        "\n",
        "        avg_time = np.mean(times) * 1000  # ms\n",
        "        std_time = np.std(times) * 1000\n",
        "        throughput = 1000.0 / avg_time  # FPS\n",
        "\n",
        "        print(f\"✓ ONNX Runtime Benchmark:\")\n",
        "        print(f\"   Average Time: {avg_time:.2f}ms (±{std_time:.2f}ms)\")\n",
        "        print(f\"   Throughput: {throughput:.2f} FPS\")\n",
        "\n",
        "        return {\n",
        "            'avg_time_ms': avg_time,\n",
        "            'std_time_ms': std_time,\n",
        "            'throughput_fps': throughput\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: TORCHSCRIPT COMPILATION\n",
        "# ============================================================================\n",
        "\n",
        "class TorchScriptCompiler:\n",
        "    \"\"\"\n",
        "    Compile model to TorchScript for optimized deployment\n",
        "    TorchScript = Optimized, portable PyTorch model format\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def compile_trace(self, input_shape=(1, 3, 640, 640)):\n",
        "        \"\"\"\n",
        "        Compile using tracing (records operations)\n",
        "        Best for: Models without control flow\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 Compiling with TorchScript (Trace)...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        dummy_input = [torch.randn(*input_shape).to(self.device)]\n",
        "\n",
        "        # Trace the model\n",
        "        with torch.no_grad():\n",
        "            traced_model = torch.jit.trace(self.model, dummy_input)\n",
        "\n",
        "        # Optimize\n",
        "        traced_model = torch.jit.optimize_for_inference(traced_model)\n",
        "\n",
        "        print(\"✓ TorchScript tracing complete!\")\n",
        "        return traced_model\n",
        "\n",
        "    def compile_script(self):\n",
        "        \"\"\"\n",
        "        Compile using scripting (analyzes Python code)\n",
        "        Best for: Models with control flow (if/for statements)\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 Compiling with TorchScript (Script)...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        scripted_model = torch.jit.script(self.model)\n",
        "\n",
        "        print(\"✓ TorchScript scripting complete!\")\n",
        "        return scripted_model\n",
        "\n",
        "    def save_torchscript(self, compiled_model, output_path=\"outputs/models/model_traced.pt\"):\n",
        "        \"\"\"Save TorchScript model\"\"\"\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        compiled_model.save(output_path)\n",
        "        file_size = Path(output_path).stat().st_size / 1e6\n",
        "\n",
        "        print(f\"✓ TorchScript model saved!\")\n",
        "        print(f\"   File: {output_path}\")\n",
        "        print(f\"   Size: {file_size:.2f}MB\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def benchmark_torchscript(self, original_model, compiled_model,\n",
        "                             num_iterations=100):\n",
        "        \"\"\"Compare original vs TorchScript model\"\"\"\n",
        "        print(\"\\n📊 Benchmarking TorchScript...\")\n",
        "\n",
        "        dummy_input = [torch.randn(1, 3, 640, 640).to(self.device)]\n",
        "\n",
        "        # Benchmark original\n",
        "        original_model.eval()\n",
        "        original_times = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                _ = original_model(dummy_input)\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                _ = original_model(dummy_input)\n",
        "                torch.cuda.synchronize()\n",
        "                original_times.append(time.time() - start)\n",
        "\n",
        "        # Benchmark compiled\n",
        "        compiled_times = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                _ = compiled_model(dummy_input)\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                torch.cuda.synchronize()\n",
        "                start = time.time()\n",
        "                _ = compiled_model(dummy_input)\n",
        "                torch.cuda.synchronize()\n",
        "                compiled_times.append(time.time() - start)\n",
        "\n",
        "        original_avg = np.mean(original_times) * 1000\n",
        "        compiled_avg = np.mean(compiled_times) * 1000\n",
        "        speedup = original_avg / compiled_avg\n",
        "\n",
        "        print(f\"\\n📊 TorchScript Results:\")\n",
        "        print(f\"   Original: {original_avg:.2f}ms\")\n",
        "        print(f\"   TorchScript: {compiled_avg:.2f}ms\")\n",
        "        print(f\"   🚀 Speedup: {speedup:.2f}x\")\n",
        "\n",
        "        return {\n",
        "            'original_time_ms': original_avg,\n",
        "            'compiled_time_ms': compiled_avg,\n",
        "            'speedup': speedup\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: BATCH PROCESSING OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "class BatchProcessingOptimizer:\n",
        "    \"\"\"\n",
        "    Optimize batch processing for maximum throughput\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "\n",
        "    def find_optimal_batch_size(self, input_shape=(3, 640, 640),\n",
        "                                max_batch_size=32, num_iterations=50):\n",
        "        \"\"\"\n",
        "        Find optimal batch size for maximum throughput\n",
        "        \"\"\"\n",
        "        print(\"\\n🔍 Finding Optimal Batch Size...\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for batch_size in [1, 2, 4, 8, 16, 32]:\n",
        "            if batch_size > max_batch_size:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                dummy_input = [torch.randn(*input_shape).to(self.device)\n",
        "                              for _ in range(batch_size)]\n",
        "\n",
        "                # Warmup\n",
        "                with torch.no_grad():\n",
        "                    for _ in range(5):\n",
        "                        _ = self.model(dummy_input)\n",
        "\n",
        "                # Benchmark\n",
        "                times = []\n",
        "                with torch.no_grad():\n",
        "                    for _ in range(num_iterations):\n",
        "                        torch.cuda.synchronize()\n",
        "                        start = time.time()\n",
        "                        _ = self.model(dummy_input)\n",
        "                        torch.cuda.synchronize()\n",
        "                        times.append(time.time() - start)\n",
        "\n",
        "                avg_time = np.mean(times)\n",
        "                throughput = batch_size / avg_time\n",
        "                latency = avg_time / batch_size\n",
        "                memory = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "                results[batch_size] = {\n",
        "                    'avg_time': avg_time,\n",
        "                    'throughput': throughput,\n",
        "                    'latency_per_image': latency,\n",
        "                    'memory_gb': memory\n",
        "                }\n",
        "\n",
        "                print(f\"   Batch {batch_size}: {throughput:.1f} img/s, \"\n",
        "                      f\"{latency*1000:.1f}ms/img, {memory:.2f}GB\")\n",
        "\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"   Batch {batch_size}: Out of memory!\")\n",
        "                    break\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        # Find optimal\n",
        "        optimal_batch = max(results.keys(),\n",
        "                           key=lambda k: results[k]['throughput'])\n",
        "\n",
        "        print(f\"\\n✓ Optimal Batch Size: {optimal_batch}\")\n",
        "        print(f\"   Throughput: {results[optimal_batch]['throughput']:.1f} img/s\")\n",
        "\n",
        "        return results, optimal_batch\n",
        "\n",
        "    def process_batch_efficiently(self, images, batch_size=8):\n",
        "        \"\"\"\n",
        "        Process list of images in optimized batches\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            batch = images[i:i+batch_size]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_results = self.model(batch)\n",
        "\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: MODEL PRUNING\n",
        "# ============================================================================\n",
        "\n",
        "class ModelPruner:\n",
        "    \"\"\"\n",
        "    Prune model to reduce size and increase speed\n",
        "    Removes less important weights\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def prune_model(self, pruning_amount=0.3):\n",
        "        \"\"\"\n",
        "        Apply structured pruning to model\n",
        "\n",
        "        Args:\n",
        "            pruning_amount: Fraction of weights to prune (0.3 = 30%)\n",
        "        \"\"\"\n",
        "        print(f\"\\n✂️ Pruning model ({pruning_amount*100:.0f}% of weights)...\")\n",
        "\n",
        "        import torch.nn.utils.prune as prune\n",
        "\n",
        "        # Count original parameters\n",
        "        original_params = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "        # Prune all Conv2d and Linear layers\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
        "                prune.remove(module, 'weight')\n",
        "\n",
        "        # Count remaining parameters\n",
        "        remaining_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        pruned_params = original_params - remaining_params\n",
        "\n",
        "        print(f\"✓ Pruning complete!\")\n",
        "        print(f\"   Original parameters: {original_params:,}\")\n",
        "        print(f\"   Pruned parameters: {pruned_params:,}\")\n",
        "        print(f\"   Reduction: {(pruned_params/original_params)*100:.1f}%\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "# ============================================================================\n",
        "# PART 6: DEPLOYMENT PACKAGE CREATOR\n",
        "# ============================================================================\n",
        "\n",
        "class DeploymentPackager:\n",
        "    \"\"\"\n",
        "    Create production-ready deployment package\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def create_deployment_package(self, output_dir=\"outputs/deployment\"):\n",
        "        \"\"\"\n",
        "        Create complete deployment package with all formats\n",
        "        \"\"\"\n",
        "        print(\"\\n📦 Creating Deployment Package...\")\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        package_info = {\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'formats': {}\n",
        "        }\n",
        "\n",
        "        # 1. Save PyTorch model\n",
        "        print(\"\\n1️⃣ Saving PyTorch model...\")\n",
        "        pytorch_path = output_path / \"model.pth\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_architecture': str(type(self.model).__name__)\n",
        "        }, pytorch_path)\n",
        "        package_info['formats']['pytorch'] = str(pytorch_path)\n",
        "        print(f\"   ✓ Saved: {pytorch_path}\")\n",
        "\n",
        "        # 2. Export to ONNX\n",
        "        print(\"\\n2️⃣ Exporting to ONNX...\")\n",
        "        try:\n",
        "            exporter = ONNXExporter(self.model, self.device)\n",
        "            onnx_path = exporter.export_to_onnx(\n",
        "                output_path=str(output_path / \"model.onnx\")\n",
        "            )\n",
        "            package_info['formats']['onnx'] = onnx_path\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ ONNX export failed: {e}\")\n",
        "\n",
        "        # 3. Compile to TorchScript\n",
        "        print(\"\\n3️⃣ Compiling to TorchScript...\")\n",
        "        try:\n",
        "            compiler = TorchScriptCompiler(self.model, self.device)\n",
        "            traced_model = compiler.compile_trace()\n",
        "            torchscript_path = compiler.save_torchscript(\n",
        "                traced_model,\n",
        "                output_path=str(output_path / \"model_traced.pt\")\n",
        "            )\n",
        "            package_info['formats']['torchscript'] = torchscript_path\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ TorchScript compilation failed: {e}\")\n",
        "\n",
        "        # 4. Create README\n",
        "        print(\"\\n4️⃣ Creating documentation...\")\n",
        "        readme_content = self._generate_readme(package_info)\n",
        "        readme_path = output_path / \"README.md\"\n",
        "        with open(readme_path, 'w') as f:\n",
        "            f.write(readme_content)\n",
        "        print(f\"   ✓ Saved: {readme_path}\")\n",
        "\n",
        "        # 5. Save package info\n",
        "        info_path = output_path / \"package_info.json\"\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(package_info, f, indent=4)\n",
        "        print(f\"   ✓ Saved: {info_path}\")\n",
        "\n",
        "        print(f\"\\n✅ Deployment package created: {output_path}\")\n",
        "\n",
        "        return package_info\n",
        "\n",
        "    def _generate_readme(self, package_info):\n",
        "        \"\"\"Generate README for deployment package\"\"\"\n",
        "        return f\"\"\"# Object Detection Model - Deployment Package\n",
        "\n",
        "## Created: {package_info['created_at']}\n",
        "\n",
        "## Available Formats\n",
        "\n",
        "### PyTorch (.pth)\n",
        "- File: `model.pth`\n",
        "- Usage:\n",
        "```python\n",
        "import torch\n",
        "checkpoint = torch.load('model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "```\n",
        "\n",
        "### ONNX (.onnx)\n",
        "- File: `model.onnx`\n",
        "- Usage:\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "session = ort.InferenceSession('model.onnx')\n",
        "output = session.run(None, {{'input': input_data}})\n",
        "```\n",
        "\n",
        "### TorchScript (.pt)\n",
        "- File: `model_traced.pt`\n",
        "- Usage:\n",
        "```python\n",
        "import torch\n",
        "model = torch.jit.load('model_traced.pt')\n",
        "output = model(input_tensor)\n",
        "```\n",
        "\n",
        "## Model Specifications\n",
        "\n",
        "- **Input**: RGB images, 640x640 pixels\n",
        "- **Output**: Bounding boxes, labels, confidence scores\n",
        "- **Classes**: 90 COCO object categories\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "```python\n",
        "# Load model\n",
        "import torch\n",
        "model = torch.jit.load('model_traced.pt')\n",
        "model.eval()\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    predictions = model([image_tensor])\n",
        "```\n",
        "\n",
        "## Performance Notes\n",
        "\n",
        "- Optimized for GPU inference\n",
        "- Supports batch processing\n",
        "- Use batch size 4-8 for best throughput\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- PyTorch >= 1.13\n",
        "- torchvision >= 0.14\n",
        "- CUDA >= 11.0 (for GPU)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# PART 7: COMPLETE PHASE 4 DEMO\n",
        "# ============================================================================\n",
        "\n",
        "def run_phase4_complete(model, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run complete Phase 4 optimization and deployment\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🚀 PHASE 4: COMPLETE OPTIMIZATION & DEPLOYMENT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'optimizations': {}\n",
        "    }\n",
        "\n",
        "    # 1. Batch Size Optimization\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 1: Batch Size Optimization\")\n",
        "    print(\"=\"*80)\n",
        "    optimizer = BatchProcessingOptimizer(model, device)\n",
        "    batch_results, optimal_batch = optimizer.find_optimal_batch_size()\n",
        "    results['optimizations']['batch_optimization'] = {\n",
        "        'results': batch_results,\n",
        "        'optimal_batch_size': optimal_batch\n",
        "    }\n",
        "\n",
        "    # 2. TorchScript Compilation\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 2: TorchScript Compilation\")\n",
        "    print(\"=\"*80)\n",
        "    compiler = TorchScriptCompiler(model, device)\n",
        "    try:\n",
        "        traced_model = compiler.compile_trace()\n",
        "        torchscript_results = compiler.benchmark_torchscript(\n",
        "            model, traced_model, num_iterations=50\n",
        "        )\n",
        "        results['optimizations']['torchscript'] = torchscript_results\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ TorchScript compilation failed: {e}\")\n",
        "        results['optimizations']['torchscript'] = {'error': str(e)}\n",
        "\n",
        "    # 3. ONNX Export\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 3: ONNX Export\")\n",
        "    print(\"=\"*80)\n",
        "    exporter = ONNXExporter(model, device)\n",
        "    try:\n",
        "        onnx_path = exporter.export_to_onnx()\n",
        "        onnx_results = exporter.benchmark_onnx(onnx_path, num_iterations=50)\n",
        "        results['optimizations']['onnx'] = onnx_results\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ ONNX export failed: {e}\")\n",
        "        results['optimizations']['onnx'] = {'error': str(e)}\n",
        "\n",
        "    # 4. Create Deployment Package\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 4: Creating Deployment Package\")\n",
        "    print(\"=\"*80)\n",
        "    packager = DeploymentPackager(model, device)\n",
        "    package_info = packager.create_deployment_package()\n",
        "    results['deployment_package'] = package_info\n",
        "\n",
        "    # 5. Save comprehensive report\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 5: Saving Performance Report\")\n",
        "    print(\"=\"*80)\n",
        "    report_path = \"outputs/metrics/phase4_optimization_report.json\"\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(results, f, indent=4, default=str)\n",
        "    print(f\"✓ Report saved: {report_path}\")\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ PHASE 4 COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n📊 Optimization Summary:\")\n",
        "    print(f\"   Optimal Batch Size: {optimal_batch}\")\n",
        "    if 'torchscript' in results['optimizations'] and 'speedup' in results['optimizations']['torchscript']:\n",
        "        print(f\"   TorchScript Speedup: {results['optimizations']['torchscript']['speedup']:.2f}x\")\n",
        "\n",
        "    print(\"\\n📁 Deployment Package:\")\n",
        "    print(\"   Location: outputs/deployment/\")\n",
        "    print(\"   Formats: PyTorch, ONNX, TorchScript\")\n",
        "    print(\"   Documentation: README.md included\")\n",
        "\n",
        "    print(\"\\n💡 Next Steps:\")\n",
        "    print(\"   1. Test deployment package on target hardware\")\n",
        "    print(\"   2. Integrate into production application\")\n",
        "    print(\"   3. Set up monitoring and logging\")\n",
        "    print(\"   4. Deploy to cloud/edge devices\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# QUICK START\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n✓ Phase 4 loaded!\")\n",
        "print(\"\\nTo run complete optimization:\")\n",
        "print(\">>> results = run_phase4_complete(model)\")\n",
        "print(\"\\nFor individual optimizations:\")\n",
        "print(\">>> optimizer = BatchProcessingOptimizer(model)\")\n",
        "print(\">>> compiler = TorchScriptCompiler(model)\")\n",
        "print(\">>> exporter = ONNXExporter(model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji_HPlWBcPWT",
        "outputId": "958b0e8b-bdf7-49f6-dd47-af49e891f619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🚀 PHASE 4: COMPLETE OPTIMIZATION & DEPLOYMENT\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Batch Size Optimization\n",
            "================================================================================\n",
            "\n",
            "🔍 Finding Optimal Batch Size...\n",
            "   Batch 1: 11.5 img/s, 86.7ms/img, 0.79GB\n",
            "   Batch 2: 10.8 img/s, 92.5ms/img, 0.72GB\n",
            "   Batch 4: 11.5 img/s, 87.3ms/img, 1.25GB\n",
            "   Batch 8: 11.9 img/s, 83.9ms/img, 5.27GB\n",
            "   Batch 16: 13.3 img/s, 75.4ms/img, 7.41GB\n",
            "   Batch 32: 13.0 img/s, 76.9ms/img, 9.14GB\n",
            "\n",
            "✓ Optimal Batch Size: 16\n",
            "   Throughput: 13.3 img/s\n",
            "\n",
            "================================================================================\n",
            "STEP 2: TorchScript Compilation\n",
            "================================================================================\n",
            "\n",
            "🔧 Compiling with TorchScript (Trace)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:4705: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  * torch.tensor(scale_factors[i], dtype=torch.float32)\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/ops/boxes.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:2174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert condition, message\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ TorchScript compilation failed: Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "\n",
            "================================================================================\n",
            "STEP 3: ONNX Export\n",
            "================================================================================\n",
            "\n",
            "📦 Exporting to ONNX...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1496072057.py:190: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ ONNX export successful!\n",
            "   File: outputs/models/model.onnx\n",
            "   Size: 167.49MB\n",
            "   Opset: 11\n",
            "\n",
            "📊 Benchmarking ONNX Runtime...\n",
            "⚠️ ONNX export failed: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running ScatterElements node. Name:'/roi_heads/box_roi_pool/ScatterElements' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 50176000\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 4: Creating Deployment Package\n",
            "================================================================================\n",
            "\n",
            "📦 Creating Deployment Package...\n",
            "\n",
            "1️⃣ Saving PyTorch model...\n",
            "   ✓ Saved: outputs/deployment/model.pth\n",
            "\n",
            "2️⃣ Exporting to ONNX...\n",
            "\n",
            "📦 Exporting to ONNX...\n",
            "✓ ONNX export successful!\n",
            "   File: outputs/deployment/model.onnx\n",
            "   Size: 167.49MB\n",
            "   Opset: 11\n",
            "\n",
            "3️⃣ Compiling to TorchScript...\n",
            "\n",
            "🔧 Compiling with TorchScript (Trace)...\n",
            "   ⚠️ TorchScript compilation failed: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "\n",
            "4️⃣ Creating documentation...\n",
            "   ✓ Saved: outputs/deployment/README.md\n",
            "   ✓ Saved: outputs/deployment/package_info.json\n",
            "\n",
            "✅ Deployment package created: outputs/deployment\n",
            "\n",
            "================================================================================\n",
            "STEP 5: Saving Performance Report\n",
            "================================================================================\n",
            "✓ Report saved: outputs/metrics/phase4_optimization_report.json\n",
            "\n",
            "================================================================================\n",
            "✅ PHASE 4 COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📊 Optimization Summary:\n",
            "   Optimal Batch Size: 16\n",
            "\n",
            "📁 Deployment Package:\n",
            "   Location: outputs/deployment/\n",
            "   Formats: PyTorch, ONNX, TorchScript\n",
            "   Documentation: README.md included\n",
            "\n",
            "💡 Next Steps:\n",
            "   1. Test deployment package on target hardware\n",
            "   2. Integrate into production application\n",
            "   3. Set up monitoring and logging\n",
            "   4. Deploy to cloud/edge devices\n"
          ]
        }
      ],
      "source": [
        "results = run_phase4_complete(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjlcULxcRGD"
      },
      "outputs": [],
      "source": [
        "optimizer = BatchProcessingOptimizer(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAS4mhSacTBj"
      },
      "outputs": [],
      "source": [
        "compiler = TorchScriptCompiler(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGh6SIg3cWzg"
      },
      "outputs": [],
      "source": [
        "exporter = ONNXExporter(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
